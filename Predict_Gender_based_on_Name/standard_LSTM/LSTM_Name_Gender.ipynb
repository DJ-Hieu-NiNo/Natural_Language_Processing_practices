{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Name_Gender.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wx3G_MFAE3P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bdec6578-84d4-4a63-8f24-bcb39b65a17a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygEgkweZAOK0",
        "colab_type": "text"
      },
      "source": [
        "# **Read Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoWIpPIQAM74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "31cceb96-1b1a-40f1-8a04-a2d7a28b1516"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset=pd.read_csv(\"/content/gdrive/My Drive/LSTM/name_gender.csv\")\n",
        "data=dataset.values\n",
        "\n",
        "n_samples=data.shape[0]-1 #delete the last row, because it will not help.\n",
        "used_data=data[:n_samples,:2] \n",
        "print(\"Used data has shape : \",used_data.shape)\n",
        "print(\"Let's show some samples : \\n\",used_data[:5])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Used data has shape :  (95025, 2)\n",
            "Let's show some samples : \n",
            " [['Aaban' 'M']\n",
            " ['Aabha' 'F']\n",
            " ['Aabid' 'M']\n",
            " ['Aabriella' 'F']\n",
            " ['Aada' 'F']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQua60YtM2lr",
        "colab_type": "text"
      },
      "source": [
        "# **Label from 'M'/'F' to 0 / 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP7rhzu6NDJl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "41f07349-b6a5-4ebf-e3f9-4fd9a33ddb38"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "m_idx=np.where(used_data[:,1]=='M')\n",
        "f_idx=np.where(used_data[:,1]=='F')\n",
        "used_data[m_idx,1]=0\n",
        "used_data[f_idx,1]=1\n",
        "print(\"Let's look some samples after converting gender labels : \",used_data[:5])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's look some samples after converting gender labels :  [['Aaban' 0]\n",
            " ['Aabha' 1]\n",
            " ['Aabid' 0]\n",
            " ['Aabriella' 1]\n",
            " ['Aada' 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94k6OuvRBJNx",
        "colab_type": "text"
      },
      "source": [
        "# **Define dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4NtML-rAe42",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2f9a648e-7178-4d9a-afc5-0dd63e7564a8"
      },
      "source": [
        "import string\n",
        "\n",
        "all_letters=string.ascii_letters # a,b,c,d.....z,A,B,C,D,...,Z\n",
        "dictionary=all_letters[:26] # only need lowercase letter\n",
        "print(\"Dictionary : \",dictionary)\n",
        "print(\"Length of dictionary : \",len(dictionary))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary :  abcdefghijklmnopqrstuvwxyz\n",
            "Length of dictionary :  26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkIOuBz5C9yU",
        "colab_type": "text"
      },
      "source": [
        "# **Convert name to torch tensor with one-hot encoding method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iS52PWLCoTe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "082fb95b-e2d5-48fe-e1e7-dc93ff1ef770"
      },
      "source": [
        "import torch\n",
        "\n",
        "n_letters=len(dictionary) #length of dict =26\n",
        "\n",
        "def name_to_tensor(name):\n",
        "  name=name.lower()\n",
        "  tensor=torch.zeros(len(name),1,n_letters) #each letter is an one-hot vector\n",
        "  for idx,letter in enumerate(name):\n",
        "    letter_idx=dictionary.find(letter)\n",
        "    tensor[idx][0][letter_idx]=1\n",
        "  return tensor\n",
        "\n",
        "#test an example\n",
        "print(\"Tensor for name : 'Hieu' : \\n\",name_to_tensor(\"Hieu\"))\n",
        "print(name_to_tensor(\"Hieu\").shape,\"\\n\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor for name : 'Hieu' : \n",
            " tensor([[[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 1., 0., 0., 0., 0., 0.]]])\n",
            "torch.Size([4, 1, 26]) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHi1cLS5GajJ",
        "colab_type": "text"
      },
      "source": [
        "# **Split Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPSAtc4vGZ6x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5f4d0334-b6c5-4b17-afa8-9a04ad5f30ba"
      },
      "source": [
        "#split\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "n_samples=len(used_data)\n",
        "idx_range=np.arange(n_samples) # Get the index from 0 to 95024 \n",
        "#np.random.shuflle(idx_range) #shuffle index\n",
        "\n",
        "X_train,X_test_val,y_train,y_test_val=train_test_split(used_data[:,0],used_data[:,1],test_size=0.2) \n",
        "X_val,X_test,y_val,y_test=train_test_split(X_test_val,y_test_val,test_size=0.5)\n",
        "\n",
        "print(\"Shape of X_train , y_train : \",X_train.shape,y_train.shape)\n",
        "print(\"Shape of X_val, y_val : \",X_val.shape,y_val.shape)\n",
        "print(\"Shape of X_test, y_test : \",X_test.shape, y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train , y_train :  (76020,) (76020,)\n",
            "Shape of X_val, y_val :  (9502,) (9502,)\n",
            "Shape of X_test, y_test :  (9503,) (9503,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "266F5PF6Khnh",
        "colab_type": "text"
      },
      "source": [
        "# **Function to load data while training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3uVICaYEHKn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8a350f14-ba35-4138-ad40-226d2d8825cb"
      },
      "source": [
        "from torch.autograd import Variable \n",
        "\n",
        "def get_data_pair(X,y,idx):\n",
        "  X_=Variable(name_to_tensor(X[idx]))\n",
        "  y_=Variable(torch.tensor([y[idx]]).long())\n",
        "  return X_,y_\n",
        "\n",
        "#test an example : \n",
        "print(X_train[100],y_train[100])\n",
        "print(\"shape X_train_tensor at 100 : \",get_data_pair(X_train,y_train,100)[0].shape)\n",
        "print(\"y_train_tensor at 100 : \",get_data_pair(X_train,y_train,100)[1].shape) \n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Srivar 0\n",
            "shape X_train_tensor at 100 :  torch.Size([6, 1, 26])\n",
            "y_train_tensor at 100 :  torch.Size([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx2BsIk0YHZd",
        "colab_type": "text"
      },
      "source": [
        "# **Define Model and send it to gpu**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1ZPobuVOiyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Apply for natural language model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#hidden_size =64\n",
        "#feature_length=26\n",
        "#=> concatenated size = 64+26=90\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size):\n",
        "    super(LSTM,self).__init__()\n",
        "    self.input_size=input_size #feature length of inputs ( Note that : inputs != input)\n",
        "    self.hidden_size=hidden_size\n",
        "    self.output_size=output_size\n",
        "    \n",
        "    #ft\n",
        "    self.neuralNet_f=nn.Linear(90,64)\n",
        "\n",
        "    #it\n",
        "    self.neuralNet_i=nn.Linear(90,64)\n",
        "\n",
        "    #ct\n",
        "    self.neuralNet_c=nn.Linear(90,64)\n",
        "\n",
        "    #ot\n",
        "    self.neuralNet_o=nn.Linear(90,64)\n",
        "\n",
        "    #logits\n",
        "    self.logits=nn.Linear(64,2)\n",
        "    self.softmax=nn.LogSoftmax()\n",
        "  \n",
        "  def forward(self,inputs):\n",
        "    sequence_length=inputs.size()[0]\n",
        "\n",
        "    h_t=torch.zeros((1,self.hidden_size))\n",
        "    c_t=torch.zeros((1,self.hidden_size))\n",
        "\n",
        "    for input_index_t in range(sequence_length):\n",
        "      x_t=inputs[input_index_t]\n",
        "      cat_t=torch.cat((x_t,h_t),1)\n",
        "\n",
        "      #ft\n",
        "      f_t=torch.sigmoid(self.neuralNet_f(cat_t))\n",
        "      \n",
        "      #it\n",
        "      i_t=torch.sigmoid(self.neuralNet_i(cat_t))\n",
        "\n",
        "      #c~t (sign as gt)\n",
        "      g_t=torch.tanh(self.neuralNet_c(cat_t))\n",
        "\n",
        "      #ot\n",
        "      o_t=torch.sigmoid(self.neuralNet_o(cat_t))\n",
        "\n",
        "      #C_t\n",
        "      c_t=f_t*c_t+i_t*g_t\n",
        "      \n",
        "      #h_t\n",
        "      h_t=o_t*torch.tanh(c_t)\n",
        "\n",
        "      #output\n",
        "      output=self.logits(h_t)\n",
        "      output=nn.LeakyReLU(0.02)(output)\n",
        "      output=self.softmax(output)\n",
        "\n",
        "    return h_t,c_t,output\n",
        "\n",
        "n_letters=len(dictionary)\n",
        "lstm=LSTM(26,64,2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_2ySfC-YXCg",
        "colab_type": "text"
      },
      "source": [
        "# **Define loss and  optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aam_Gbey7v7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion=nn.NLLLoss()\n",
        "learning_rate=0.001\n",
        "optimizer=torch.optim.SGD(lstm.parameters(),lr=learning_rate)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XWIUNFJhdTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(name_tensor,class_tensor):\n",
        "  lstm.zero_grad()\n",
        "  output=lstm(name_tensor)[2]\n",
        "  loss=criterion(output,class_tensor)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return output,loss.item()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jiyokM0hulG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "36a14498-16ad-43cf-e30b-ef13c766a138"
      },
      "source": [
        "epochs = 10\n",
        "print_steps=1000\n",
        "print_all_steps=75000\n",
        "indices_train=np.arange(len(X_train))\n",
        "train_loss_list=[]\n",
        "train_acc_list=[]\n",
        "val_loss_list=[]\n",
        "val_acc_list=[]\n",
        "\n",
        "def evaluate(X,y):\n",
        "  loss=0\n",
        "  correct=0\n",
        "  for i in range(len(X)):\n",
        "    name_tensor,class_tensor=get_data_pair(X,y,i)\n",
        "    predicted=lstm(name_tensor)[2]\n",
        "    loss+=criterion(predicted,class_tensor)\n",
        "    idx_predicted=torch.max(predicted.data,1)[1]\n",
        "    correct+=(idx_predicted==class_tensor).sum()\n",
        "  return loss/len(X),correct.item()/len(X)*100\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss=0\n",
        "  np.random.shuffle(indices_train)\n",
        "  for i,idx in enumerate(indices_train):\n",
        "    name_tensor,class_tensor=get_data_pair(X_train,y_train,idx)\n",
        "    output,loss=train(name_tensor,class_tensor)\n",
        "    train_loss+=loss\n",
        "    if(i%print_steps==0 and i>0 and i%print_all_steps !=0):\n",
        "      print(\"Iter :\",i,\"at epoch: \",epoch+1,\"/ \",epochs,\" Train Loss : \",train_loss/print_steps)\n",
        "      train_loss=0\n",
        "    elif(i%print_all_steps==0 and i>0):\n",
        "      with torch.no_grad():\n",
        "\n",
        "        train_acc=evaluate(X_train,y_train)[1] #Calculate acc for all\n",
        "        train_acc_list.append(train_acc)\n",
        "        train_loss_list.append(train_loss/print_steps) #calculate loss after \"print_steps\" training samples and get mean\n",
        "        \n",
        "        val_loss,val_acc=evaluate(X_val,y_val)\n",
        "        val_loss_list.append(val_loss)\n",
        "        val_acc_list.append(val_acc)\n",
        "        print(\"Iter: \",i,\"at epoch: \",epoch+1,\"/\",epochs,\" train loss :\",train_loss/print_steps,\"Train acc: \",train_acc,\" val loss: \",val_loss,\" val acc: \",val_acc)\n",
        "        train_loss=0"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter : 1000 at epoch:  1 /  10  Train Loss :  0.6939919663667679\n",
            "Iter : 2000 at epoch:  1 /  10  Train Loss :  0.6930539111495018\n",
            "Iter : 3000 at epoch:  1 /  10  Train Loss :  0.6895568848252297\n",
            "Iter : 4000 at epoch:  1 /  10  Train Loss :  0.6758558031320572\n",
            "Iter : 5000 at epoch:  1 /  10  Train Loss :  0.6633740720748902\n",
            "Iter : 6000 at epoch:  1 /  10  Train Loss :  0.6556526923179626\n",
            "Iter : 7000 at epoch:  1 /  10  Train Loss :  0.6500881291329861\n",
            "Iter : 8000 at epoch:  1 /  10  Train Loss :  0.6536449889540672\n",
            "Iter : 9000 at epoch:  1 /  10  Train Loss :  0.6503292200565338\n",
            "Iter : 10000 at epoch:  1 /  10  Train Loss :  0.6512006373107433\n",
            "Iter : 11000 at epoch:  1 /  10  Train Loss :  0.6531001300513745\n",
            "Iter : 12000 at epoch:  1 /  10  Train Loss :  0.645011263370514\n",
            "Iter : 13000 at epoch:  1 /  10  Train Loss :  0.6566026080846786\n",
            "Iter : 14000 at epoch:  1 /  10  Train Loss :  0.6318392823338509\n",
            "Iter : 15000 at epoch:  1 /  10  Train Loss :  0.6566556480824948\n",
            "Iter : 16000 at epoch:  1 /  10  Train Loss :  0.6492818384766579\n",
            "Iter : 17000 at epoch:  1 /  10  Train Loss :  0.661553844243288\n",
            "Iter : 18000 at epoch:  1 /  10  Train Loss :  0.6343015798926354\n",
            "Iter : 19000 at epoch:  1 /  10  Train Loss :  0.6443527263998985\n",
            "Iter : 20000 at epoch:  1 /  10  Train Loss :  0.6367876370549201\n",
            "Iter : 21000 at epoch:  1 /  10  Train Loss :  0.6531949819028378\n",
            "Iter : 22000 at epoch:  1 /  10  Train Loss :  0.6273257511258126\n",
            "Iter : 23000 at epoch:  1 /  10  Train Loss :  0.6366416240632534\n",
            "Iter : 24000 at epoch:  1 /  10  Train Loss :  0.6283366828262806\n",
            "Iter : 25000 at epoch:  1 /  10  Train Loss :  0.6193334206044674\n",
            "Iter : 26000 at epoch:  1 /  10  Train Loss :  0.6123902958929539\n",
            "Iter : 27000 at epoch:  1 /  10  Train Loss :  0.6311601957678795\n",
            "Iter : 28000 at epoch:  1 /  10  Train Loss :  0.6093317580521107\n",
            "Iter : 29000 at epoch:  1 /  10  Train Loss :  0.6232208827733994\n",
            "Iter : 30000 at epoch:  1 /  10  Train Loss :  0.6110564664900303\n",
            "Iter : 31000 at epoch:  1 /  10  Train Loss :  0.6232410590946674\n",
            "Iter : 32000 at epoch:  1 /  10  Train Loss :  0.617594092130661\n",
            "Iter : 33000 at epoch:  1 /  10  Train Loss :  0.6138619906306266\n",
            "Iter : 34000 at epoch:  1 /  10  Train Loss :  0.6126700874567031\n",
            "Iter : 35000 at epoch:  1 /  10  Train Loss :  0.6212722093462945\n",
            "Iter : 36000 at epoch:  1 /  10  Train Loss :  0.6156932027041913\n",
            "Iter : 37000 at epoch:  1 /  10  Train Loss :  0.6058543172776699\n",
            "Iter : 38000 at epoch:  1 /  10  Train Loss :  0.5993243037164211\n",
            "Iter : 39000 at epoch:  1 /  10  Train Loss :  0.6025612310767173\n",
            "Iter : 40000 at epoch:  1 /  10  Train Loss :  0.5854390853345394\n",
            "Iter : 41000 at epoch:  1 /  10  Train Loss :  0.5849923973381519\n",
            "Iter : 42000 at epoch:  1 /  10  Train Loss :  0.5941773756742478\n",
            "Iter : 43000 at epoch:  1 /  10  Train Loss :  0.593123563528061\n",
            "Iter : 44000 at epoch:  1 /  10  Train Loss :  0.5830564393997192\n",
            "Iter : 45000 at epoch:  1 /  10  Train Loss :  0.5862790034115315\n",
            "Iter : 46000 at epoch:  1 /  10  Train Loss :  0.587090406909585\n",
            "Iter : 47000 at epoch:  1 /  10  Train Loss :  0.5781347099840641\n",
            "Iter : 48000 at epoch:  1 /  10  Train Loss :  0.5782226666808128\n",
            "Iter : 49000 at epoch:  1 /  10  Train Loss :  0.567309588715434\n",
            "Iter : 50000 at epoch:  1 /  10  Train Loss :  0.5726117074638605\n",
            "Iter : 51000 at epoch:  1 /  10  Train Loss :  0.5656960738152266\n",
            "Iter : 52000 at epoch:  1 /  10  Train Loss :  0.566122745424509\n",
            "Iter : 53000 at epoch:  1 /  10  Train Loss :  0.5485503819137811\n",
            "Iter : 54000 at epoch:  1 /  10  Train Loss :  0.5586379854232073\n",
            "Iter : 55000 at epoch:  1 /  10  Train Loss :  0.5516693484187126\n",
            "Iter : 56000 at epoch:  1 /  10  Train Loss :  0.5432766314744949\n",
            "Iter : 57000 at epoch:  1 /  10  Train Loss :  0.5437204765826463\n",
            "Iter : 58000 at epoch:  1 /  10  Train Loss :  0.5479448120296001\n",
            "Iter : 59000 at epoch:  1 /  10  Train Loss :  0.5648617881089449\n",
            "Iter : 60000 at epoch:  1 /  10  Train Loss :  0.5407311596423388\n",
            "Iter : 61000 at epoch:  1 /  10  Train Loss :  0.5467060485035181\n",
            "Iter : 62000 at epoch:  1 /  10  Train Loss :  0.5522055574804544\n",
            "Iter : 63000 at epoch:  1 /  10  Train Loss :  0.5401859335750342\n",
            "Iter : 64000 at epoch:  1 /  10  Train Loss :  0.5186799448281526\n",
            "Iter : 65000 at epoch:  1 /  10  Train Loss :  0.522044921502471\n",
            "Iter : 66000 at epoch:  1 /  10  Train Loss :  0.5181372778713703\n",
            "Iter : 67000 at epoch:  1 /  10  Train Loss :  0.5204507613703608\n",
            "Iter : 68000 at epoch:  1 /  10  Train Loss :  0.5134233148247004\n",
            "Iter : 69000 at epoch:  1 /  10  Train Loss :  0.503927207224071\n",
            "Iter : 70000 at epoch:  1 /  10  Train Loss :  0.517306597225368\n",
            "Iter : 71000 at epoch:  1 /  10  Train Loss :  0.5166247277259827\n",
            "Iter : 72000 at epoch:  1 /  10  Train Loss :  0.5161416752040386\n",
            "Iter : 73000 at epoch:  1 /  10  Train Loss :  0.5128707839176059\n",
            "Iter : 74000 at epoch:  1 /  10  Train Loss :  0.5146041470170021\n",
            "Iter:  75000 at epoch:  1 / 10  train loss : 0.5017422570809722 Train acc:  74.56064193633254  val loss:  tensor(0.4976)  val acc:  74.67901494422226\n",
            "Iter : 76000 at epoch:  1 /  10  Train Loss :  0.4905183158069849\n",
            "Iter : 1000 at epoch:  2 /  10  Train Loss :  0.471593943297863\n",
            "Iter : 2000 at epoch:  2 /  10  Train Loss :  0.5035529387444257\n",
            "Iter : 3000 at epoch:  2 /  10  Train Loss :  0.489924177467823\n",
            "Iter : 4000 at epoch:  2 /  10  Train Loss :  0.4793084634691477\n",
            "Iter : 5000 at epoch:  2 /  10  Train Loss :  0.4813959318473935\n",
            "Iter : 6000 at epoch:  2 /  10  Train Loss :  0.48551922238618134\n",
            "Iter : 7000 at epoch:  2 /  10  Train Loss :  0.49407923989742997\n",
            "Iter : 8000 at epoch:  2 /  10  Train Loss :  0.4394791417047381\n",
            "Iter : 9000 at epoch:  2 /  10  Train Loss :  0.4713083501011133\n",
            "Iter : 10000 at epoch:  2 /  10  Train Loss :  0.47769483081251385\n",
            "Iter : 11000 at epoch:  2 /  10  Train Loss :  0.4649363994821906\n",
            "Iter : 12000 at epoch:  2 /  10  Train Loss :  0.4569768525213003\n",
            "Iter : 13000 at epoch:  2 /  10  Train Loss :  0.48290462253987787\n",
            "Iter : 14000 at epoch:  2 /  10  Train Loss :  0.45597658006846903\n",
            "Iter : 15000 at epoch:  2 /  10  Train Loss :  0.46151573907583954\n",
            "Iter : 16000 at epoch:  2 /  10  Train Loss :  0.46243891862779857\n",
            "Iter : 17000 at epoch:  2 /  10  Train Loss :  0.4611358398720622\n",
            "Iter : 18000 at epoch:  2 /  10  Train Loss :  0.4625669964812696\n",
            "Iter : 19000 at epoch:  2 /  10  Train Loss :  0.4566614214107394\n",
            "Iter : 20000 at epoch:  2 /  10  Train Loss :  0.4715638498440385\n",
            "Iter : 21000 at epoch:  2 /  10  Train Loss :  0.47427475897967813\n",
            "Iter : 22000 at epoch:  2 /  10  Train Loss :  0.46350091898813844\n",
            "Iter : 23000 at epoch:  2 /  10  Train Loss :  0.46783281107619407\n",
            "Iter : 24000 at epoch:  2 /  10  Train Loss :  0.4685398694202304\n",
            "Iter : 25000 at epoch:  2 /  10  Train Loss :  0.45254133927077056\n",
            "Iter : 26000 at epoch:  2 /  10  Train Loss :  0.4529132280126214\n",
            "Iter : 27000 at epoch:  2 /  10  Train Loss :  0.4461902436427772\n",
            "Iter : 28000 at epoch:  2 /  10  Train Loss :  0.45941510816663506\n",
            "Iter : 29000 at epoch:  2 /  10  Train Loss :  0.43091340640559794\n",
            "Iter : 30000 at epoch:  2 /  10  Train Loss :  0.4331704502403736\n",
            "Iter : 31000 at epoch:  2 /  10  Train Loss :  0.4418530145660043\n",
            "Iter : 32000 at epoch:  2 /  10  Train Loss :  0.43249611656740305\n",
            "Iter : 33000 at epoch:  2 /  10  Train Loss :  0.4115941843688488\n",
            "Iter : 34000 at epoch:  2 /  10  Train Loss :  0.43084936881437896\n",
            "Iter : 35000 at epoch:  2 /  10  Train Loss :  0.44300740348175166\n",
            "Iter : 36000 at epoch:  2 /  10  Train Loss :  0.4260902553386986\n",
            "Iter : 37000 at epoch:  2 /  10  Train Loss :  0.42531432523950935\n",
            "Iter : 38000 at epoch:  2 /  10  Train Loss :  0.44652527866140007\n",
            "Iter : 39000 at epoch:  2 /  10  Train Loss :  0.41820675048977135\n",
            "Iter : 40000 at epoch:  2 /  10  Train Loss :  0.4371464360691607\n",
            "Iter : 41000 at epoch:  2 /  10  Train Loss :  0.43913548988848927\n",
            "Iter : 42000 at epoch:  2 /  10  Train Loss :  0.42597142301499846\n",
            "Iter : 43000 at epoch:  2 /  10  Train Loss :  0.439283658888191\n",
            "Iter : 44000 at epoch:  2 /  10  Train Loss :  0.4116465512514114\n",
            "Iter : 45000 at epoch:  2 /  10  Train Loss :  0.4187538934275508\n",
            "Iter : 46000 at epoch:  2 /  10  Train Loss :  0.42958219135925174\n",
            "Iter : 47000 at epoch:  2 /  10  Train Loss :  0.4168050313256681\n",
            "Iter : 48000 at epoch:  2 /  10  Train Loss :  0.4187998046763241\n",
            "Iter : 49000 at epoch:  2 /  10  Train Loss :  0.4467203057780862\n",
            "Iter : 50000 at epoch:  2 /  10  Train Loss :  0.4090839087385684\n",
            "Iter : 51000 at epoch:  2 /  10  Train Loss :  0.42155592950060966\n",
            "Iter : 52000 at epoch:  2 /  10  Train Loss :  0.44140406654402614\n",
            "Iter : 53000 at epoch:  2 /  10  Train Loss :  0.4174029513131827\n",
            "Iter : 54000 at epoch:  2 /  10  Train Loss :  0.40996291359327736\n",
            "Iter : 55000 at epoch:  2 /  10  Train Loss :  0.41875825884751977\n",
            "Iter : 56000 at epoch:  2 /  10  Train Loss :  0.4134009595867246\n",
            "Iter : 57000 at epoch:  2 /  10  Train Loss :  0.407724121209234\n",
            "Iter : 58000 at epoch:  2 /  10  Train Loss :  0.4200181059483439\n",
            "Iter : 59000 at epoch:  2 /  10  Train Loss :  0.4365748028550297\n",
            "Iter : 60000 at epoch:  2 /  10  Train Loss :  0.411727913018316\n",
            "Iter : 61000 at epoch:  2 /  10  Train Loss :  0.40356895511969926\n",
            "Iter : 62000 at epoch:  2 /  10  Train Loss :  0.43766634712740776\n",
            "Iter : 63000 at epoch:  2 /  10  Train Loss :  0.42453794290311636\n",
            "Iter : 64000 at epoch:  2 /  10  Train Loss :  0.3976891692336649\n",
            "Iter : 65000 at epoch:  2 /  10  Train Loss :  0.4260517081990838\n",
            "Iter : 66000 at epoch:  2 /  10  Train Loss :  0.4061971413493156\n",
            "Iter : 67000 at epoch:  2 /  10  Train Loss :  0.3978018176332116\n",
            "Iter : 68000 at epoch:  2 /  10  Train Loss :  0.4053775199037045\n",
            "Iter : 69000 at epoch:  2 /  10  Train Loss :  0.40870569194853307\n",
            "Iter : 70000 at epoch:  2 /  10  Train Loss :  0.39683069032803175\n",
            "Iter : 71000 at epoch:  2 /  10  Train Loss :  0.4235867191366851\n",
            "Iter : 72000 at epoch:  2 /  10  Train Loss :  0.43919228092208507\n",
            "Iter : 73000 at epoch:  2 /  10  Train Loss :  0.383458134803921\n",
            "Iter : 74000 at epoch:  2 /  10  Train Loss :  0.40843221003562213\n",
            "Iter:  75000 at epoch:  2 / 10  train loss : 0.4072554803788662 Train acc:  82.07313864772429  val loss:  tensor(0.4015)  val acc:  82.29846348137234\n",
            "Iter : 76000 at epoch:  2 /  10  Train Loss :  0.37449057810008524\n",
            "Iter : 1000 at epoch:  3 /  10  Train Loss :  0.40968669738993047\n",
            "Iter : 2000 at epoch:  3 /  10  Train Loss :  0.4145202738195658\n",
            "Iter : 3000 at epoch:  3 /  10  Train Loss :  0.38547599413152783\n",
            "Iter : 4000 at epoch:  3 /  10  Train Loss :  0.4166896815886721\n",
            "Iter : 5000 at epoch:  3 /  10  Train Loss :  0.4154462186777964\n",
            "Iter : 6000 at epoch:  3 /  10  Train Loss :  0.40257839892432096\n",
            "Iter : 7000 at epoch:  3 /  10  Train Loss :  0.3802100141094997\n",
            "Iter : 8000 at epoch:  3 /  10  Train Loss :  0.4148254001913592\n",
            "Iter : 9000 at epoch:  3 /  10  Train Loss :  0.41624554682709275\n",
            "Iter : 10000 at epoch:  3 /  10  Train Loss :  0.4362979273153469\n",
            "Iter : 11000 at epoch:  3 /  10  Train Loss :  0.3936365526355803\n",
            "Iter : 12000 at epoch:  3 /  10  Train Loss :  0.3990593957062811\n",
            "Iter : 13000 at epoch:  3 /  10  Train Loss :  0.3931737342728302\n",
            "Iter : 14000 at epoch:  3 /  10  Train Loss :  0.39111616160348056\n",
            "Iter : 15000 at epoch:  3 /  10  Train Loss :  0.41632536841277035\n",
            "Iter : 16000 at epoch:  3 /  10  Train Loss :  0.38255418255459517\n",
            "Iter : 17000 at epoch:  3 /  10  Train Loss :  0.399453673937358\n",
            "Iter : 18000 at epoch:  3 /  10  Train Loss :  0.3998925784016028\n",
            "Iter : 19000 at epoch:  3 /  10  Train Loss :  0.3890622308561578\n",
            "Iter : 20000 at epoch:  3 /  10  Train Loss :  0.41477665134519337\n",
            "Iter : 21000 at epoch:  3 /  10  Train Loss :  0.42471962415706366\n",
            "Iter : 22000 at epoch:  3 /  10  Train Loss :  0.37535772823821756\n",
            "Iter : 23000 at epoch:  3 /  10  Train Loss :  0.3772148355748504\n",
            "Iter : 24000 at epoch:  3 /  10  Train Loss :  0.4045847094366327\n",
            "Iter : 25000 at epoch:  3 /  10  Train Loss :  0.3945024657184258\n",
            "Iter : 26000 at epoch:  3 /  10  Train Loss :  0.4120110354498029\n",
            "Iter : 27000 at epoch:  3 /  10  Train Loss :  0.39044170715194193\n",
            "Iter : 28000 at epoch:  3 /  10  Train Loss :  0.4189498296417296\n",
            "Iter : 29000 at epoch:  3 /  10  Train Loss :  0.4160528774857521\n",
            "Iter : 30000 at epoch:  3 /  10  Train Loss :  0.4285676315482706\n",
            "Iter : 31000 at epoch:  3 /  10  Train Loss :  0.3977278498383239\n",
            "Iter : 32000 at epoch:  3 /  10  Train Loss :  0.38455884547065944\n",
            "Iter : 33000 at epoch:  3 /  10  Train Loss :  0.3826611567661166\n",
            "Iter : 34000 at epoch:  3 /  10  Train Loss :  0.39680028140358625\n",
            "Iter : 35000 at epoch:  3 /  10  Train Loss :  0.37919552861247213\n",
            "Iter : 36000 at epoch:  3 /  10  Train Loss :  0.3967106341533363\n",
            "Iter : 37000 at epoch:  3 /  10  Train Loss :  0.40789140362944454\n",
            "Iter : 38000 at epoch:  3 /  10  Train Loss :  0.4165542655242607\n",
            "Iter : 39000 at epoch:  3 /  10  Train Loss :  0.37743426581379025\n",
            "Iter : 40000 at epoch:  3 /  10  Train Loss :  0.3881782825384289\n",
            "Iter : 41000 at epoch:  3 /  10  Train Loss :  0.39883869404997674\n",
            "Iter : 42000 at epoch:  3 /  10  Train Loss :  0.3907952090641484\n",
            "Iter : 43000 at epoch:  3 /  10  Train Loss :  0.3811863365918398\n",
            "Iter : 44000 at epoch:  3 /  10  Train Loss :  0.42826687850337475\n",
            "Iter : 45000 at epoch:  3 /  10  Train Loss :  0.39406887771468607\n",
            "Iter : 46000 at epoch:  3 /  10  Train Loss :  0.3798568204408512\n",
            "Iter : 47000 at epoch:  3 /  10  Train Loss :  0.37934829051885754\n",
            "Iter : 48000 at epoch:  3 /  10  Train Loss :  0.3826235409146175\n",
            "Iter : 49000 at epoch:  3 /  10  Train Loss :  0.3621674037249759\n",
            "Iter : 50000 at epoch:  3 /  10  Train Loss :  0.38968051759526134\n",
            "Iter : 51000 at epoch:  3 /  10  Train Loss :  0.3761538718845695\n",
            "Iter : 52000 at epoch:  3 /  10  Train Loss :  0.38734460082370786\n",
            "Iter : 53000 at epoch:  3 /  10  Train Loss :  0.38772193062491717\n",
            "Iter : 54000 at epoch:  3 /  10  Train Loss :  0.41399479542858897\n",
            "Iter : 55000 at epoch:  3 /  10  Train Loss :  0.39789716992806645\n",
            "Iter : 56000 at epoch:  3 /  10  Train Loss :  0.3878808179879561\n",
            "Iter : 57000 at epoch:  3 /  10  Train Loss :  0.40267961330991237\n",
            "Iter : 58000 at epoch:  3 /  10  Train Loss :  0.39017319441726434\n",
            "Iter : 59000 at epoch:  3 /  10  Train Loss :  0.38655475293379277\n",
            "Iter : 60000 at epoch:  3 /  10  Train Loss :  0.3998556636720896\n",
            "Iter : 61000 at epoch:  3 /  10  Train Loss :  0.37697190783638507\n",
            "Iter : 62000 at epoch:  3 /  10  Train Loss :  0.4164367893859744\n",
            "Iter : 63000 at epoch:  3 /  10  Train Loss :  0.38558602159284056\n",
            "Iter : 64000 at epoch:  3 /  10  Train Loss :  0.3964316156469285\n",
            "Iter : 65000 at epoch:  3 /  10  Train Loss :  0.4052311878893524\n",
            "Iter : 66000 at epoch:  3 /  10  Train Loss :  0.3846674955757335\n",
            "Iter : 67000 at epoch:  3 /  10  Train Loss :  0.40558728165831415\n",
            "Iter : 68000 at epoch:  3 /  10  Train Loss :  0.41266039670910687\n",
            "Iter : 69000 at epoch:  3 /  10  Train Loss :  0.3781898357924074\n",
            "Iter : 70000 at epoch:  3 /  10  Train Loss :  0.38078173038549723\n",
            "Iter : 71000 at epoch:  3 /  10  Train Loss :  0.36658076299354436\n",
            "Iter : 72000 at epoch:  3 /  10  Train Loss :  0.36983393371291456\n",
            "Iter : 73000 at epoch:  3 /  10  Train Loss :  0.4052295907251537\n",
            "Iter : 74000 at epoch:  3 /  10  Train Loss :  0.34972826225310566\n",
            "Iter:  75000 at epoch:  3 / 10  train loss : 0.36716293333098293 Train acc:  82.4782951854775  val loss:  tensor(0.3891)  val acc:  82.9088612923595\n",
            "Iter : 76000 at epoch:  3 /  10  Train Loss :  0.3769451403683051\n",
            "Iter : 1000 at epoch:  4 /  10  Train Loss :  0.3969322919016704\n",
            "Iter : 2000 at epoch:  4 /  10  Train Loss :  0.3959043901367113\n",
            "Iter : 3000 at epoch:  4 /  10  Train Loss :  0.38365655826497824\n",
            "Iter : 4000 at epoch:  4 /  10  Train Loss :  0.40192045148741457\n",
            "Iter : 5000 at epoch:  4 /  10  Train Loss :  0.42054215026367453\n",
            "Iter : 6000 at epoch:  4 /  10  Train Loss :  0.40494470481947065\n",
            "Iter : 7000 at epoch:  4 /  10  Train Loss :  0.3987844242909923\n",
            "Iter : 8000 at epoch:  4 /  10  Train Loss :  0.3801019470207393\n",
            "Iter : 9000 at epoch:  4 /  10  Train Loss :  0.37631147280242294\n",
            "Iter : 10000 at epoch:  4 /  10  Train Loss :  0.3774174334425479\n",
            "Iter : 11000 at epoch:  4 /  10  Train Loss :  0.3965826937686652\n",
            "Iter : 12000 at epoch:  4 /  10  Train Loss :  0.39511414522584526\n",
            "Iter : 13000 at epoch:  4 /  10  Train Loss :  0.4176904267333448\n",
            "Iter : 14000 at epoch:  4 /  10  Train Loss :  0.35292494180612266\n",
            "Iter : 15000 at epoch:  4 /  10  Train Loss :  0.3847699823793955\n",
            "Iter : 16000 at epoch:  4 /  10  Train Loss :  0.41125286227837204\n",
            "Iter : 17000 at epoch:  4 /  10  Train Loss :  0.40268384536448865\n",
            "Iter : 18000 at epoch:  4 /  10  Train Loss :  0.39855369726382195\n",
            "Iter : 19000 at epoch:  4 /  10  Train Loss :  0.3689900419898331\n",
            "Iter : 20000 at epoch:  4 /  10  Train Loss :  0.404931187632028\n",
            "Iter : 21000 at epoch:  4 /  10  Train Loss :  0.4083208861397579\n",
            "Iter : 22000 at epoch:  4 /  10  Train Loss :  0.3900839389245957\n",
            "Iter : 23000 at epoch:  4 /  10  Train Loss :  0.40214340334013104\n",
            "Iter : 24000 at epoch:  4 /  10  Train Loss :  0.3956245366567746\n",
            "Iter : 25000 at epoch:  4 /  10  Train Loss :  0.38164337889198213\n",
            "Iter : 26000 at epoch:  4 /  10  Train Loss :  0.3917499386658892\n",
            "Iter : 27000 at epoch:  4 /  10  Train Loss :  0.4223002596171573\n",
            "Iter : 28000 at epoch:  4 /  10  Train Loss :  0.37816604583431035\n",
            "Iter : 29000 at epoch:  4 /  10  Train Loss :  0.37371038393024353\n",
            "Iter : 30000 at epoch:  4 /  10  Train Loss :  0.36017109354492277\n",
            "Iter : 31000 at epoch:  4 /  10  Train Loss :  0.3767485070577823\n",
            "Iter : 32000 at epoch:  4 /  10  Train Loss :  0.3983414168776944\n",
            "Iter : 33000 at epoch:  4 /  10  Train Loss :  0.3873168498603627\n",
            "Iter : 34000 at epoch:  4 /  10  Train Loss :  0.3779008409259841\n",
            "Iter : 35000 at epoch:  4 /  10  Train Loss :  0.39017734812758864\n",
            "Iter : 36000 at epoch:  4 /  10  Train Loss :  0.38590541047649457\n",
            "Iter : 37000 at epoch:  4 /  10  Train Loss :  0.4077409948850982\n",
            "Iter : 38000 at epoch:  4 /  10  Train Loss :  0.3614518714086153\n",
            "Iter : 39000 at epoch:  4 /  10  Train Loss :  0.39406704194098713\n",
            "Iter : 40000 at epoch:  4 /  10  Train Loss :  0.3792364785075188\n",
            "Iter : 41000 at epoch:  4 /  10  Train Loss :  0.40366287337010726\n",
            "Iter : 42000 at epoch:  4 /  10  Train Loss :  0.4325804894999601\n",
            "Iter : 43000 at epoch:  4 /  10  Train Loss :  0.38670147403609006\n",
            "Iter : 44000 at epoch:  4 /  10  Train Loss :  0.36312924250541256\n",
            "Iter : 45000 at epoch:  4 /  10  Train Loss :  0.40636222911719233\n",
            "Iter : 46000 at epoch:  4 /  10  Train Loss :  0.39119790425803513\n",
            "Iter : 47000 at epoch:  4 /  10  Train Loss :  0.3722412264999002\n",
            "Iter : 48000 at epoch:  4 /  10  Train Loss :  0.3487763843657449\n",
            "Iter : 49000 at epoch:  4 /  10  Train Loss :  0.3735436222930439\n",
            "Iter : 50000 at epoch:  4 /  10  Train Loss :  0.40908927386067806\n",
            "Iter : 51000 at epoch:  4 /  10  Train Loss :  0.37374678609939294\n",
            "Iter : 52000 at epoch:  4 /  10  Train Loss :  0.37611617455165836\n",
            "Iter : 53000 at epoch:  4 /  10  Train Loss :  0.41262368933856486\n",
            "Iter : 54000 at epoch:  4 /  10  Train Loss :  0.40727808384923264\n",
            "Iter : 55000 at epoch:  4 /  10  Train Loss :  0.37610209309030324\n",
            "Iter : 56000 at epoch:  4 /  10  Train Loss :  0.410112976456061\n",
            "Iter : 57000 at epoch:  4 /  10  Train Loss :  0.3848683581058867\n",
            "Iter : 58000 at epoch:  4 /  10  Train Loss :  0.3611363685708493\n",
            "Iter : 59000 at epoch:  4 /  10  Train Loss :  0.3785712794638239\n",
            "Iter : 60000 at epoch:  4 /  10  Train Loss :  0.37766549648018555\n",
            "Iter : 61000 at epoch:  4 /  10  Train Loss :  0.39115096464101223\n",
            "Iter : 62000 at epoch:  4 /  10  Train Loss :  0.3882761967945844\n",
            "Iter : 63000 at epoch:  4 /  10  Train Loss :  0.3911837960081175\n",
            "Iter : 64000 at epoch:  4 /  10  Train Loss :  0.3867105564540252\n",
            "Iter : 65000 at epoch:  4 /  10  Train Loss :  0.432759510663338\n",
            "Iter : 66000 at epoch:  4 /  10  Train Loss :  0.37427595442160966\n",
            "Iter : 67000 at epoch:  4 /  10  Train Loss :  0.3851408558478579\n",
            "Iter : 68000 at epoch:  4 /  10  Train Loss :  0.40299832534324376\n",
            "Iter : 69000 at epoch:  4 /  10  Train Loss :  0.37602345991972835\n",
            "Iter : 70000 at epoch:  4 /  10  Train Loss :  0.3695168863190338\n",
            "Iter : 71000 at epoch:  4 /  10  Train Loss :  0.3702804776644334\n",
            "Iter : 72000 at epoch:  4 /  10  Train Loss :  0.3821263613612391\n",
            "Iter : 73000 at epoch:  4 /  10  Train Loss :  0.3837738563939929\n",
            "Iter : 74000 at epoch:  4 /  10  Train Loss :  0.4018356548305601\n",
            "Iter:  75000 at epoch:  4 / 10  train loss : 0.37155829745763913 Train acc:  82.40726124704025  val loss:  tensor(0.3908)  val acc:  82.48789728478215\n",
            "Iter : 76000 at epoch:  4 /  10  Train Loss :  0.3957173968786374\n",
            "Iter : 1000 at epoch:  5 /  10  Train Loss :  0.3693295813249424\n",
            "Iter : 2000 at epoch:  5 /  10  Train Loss :  0.38961689586285503\n",
            "Iter : 3000 at epoch:  5 /  10  Train Loss :  0.39102143118577076\n",
            "Iter : 4000 at epoch:  5 /  10  Train Loss :  0.39244105968298393\n",
            "Iter : 5000 at epoch:  5 /  10  Train Loss :  0.4090313402689062\n",
            "Iter : 6000 at epoch:  5 /  10  Train Loss :  0.3939658170663752\n",
            "Iter : 7000 at epoch:  5 /  10  Train Loss :  0.37613284640666095\n",
            "Iter : 8000 at epoch:  5 /  10  Train Loss :  0.4099967701854184\n",
            "Iter : 9000 at epoch:  5 /  10  Train Loss :  0.38862305420264603\n",
            "Iter : 10000 at epoch:  5 /  10  Train Loss :  0.4095879643028602\n",
            "Iter : 11000 at epoch:  5 /  10  Train Loss :  0.4027275569681078\n",
            "Iter : 12000 at epoch:  5 /  10  Train Loss :  0.39385014593368395\n",
            "Iter : 13000 at epoch:  5 /  10  Train Loss :  0.3682003886885941\n",
            "Iter : 14000 at epoch:  5 /  10  Train Loss :  0.37064429618045686\n",
            "Iter : 15000 at epoch:  5 /  10  Train Loss :  0.39650558677269143\n",
            "Iter : 16000 at epoch:  5 /  10  Train Loss :  0.36602611145563424\n",
            "Iter : 17000 at epoch:  5 /  10  Train Loss :  0.3895116602126509\n",
            "Iter : 18000 at epoch:  5 /  10  Train Loss :  0.3624729176564142\n",
            "Iter : 19000 at epoch:  5 /  10  Train Loss :  0.3812399242911488\n",
            "Iter : 20000 at epoch:  5 /  10  Train Loss :  0.40315045572118835\n",
            "Iter : 21000 at epoch:  5 /  10  Train Loss :  0.3578885430600494\n",
            "Iter : 22000 at epoch:  5 /  10  Train Loss :  0.3912244358053431\n",
            "Iter : 23000 at epoch:  5 /  10  Train Loss :  0.3712934195562266\n",
            "Iter : 24000 at epoch:  5 /  10  Train Loss :  0.39021545261517165\n",
            "Iter : 25000 at epoch:  5 /  10  Train Loss :  0.3718156347097829\n",
            "Iter : 26000 at epoch:  5 /  10  Train Loss :  0.3969044181429781\n",
            "Iter : 27000 at epoch:  5 /  10  Train Loss :  0.4114425176167861\n",
            "Iter : 28000 at epoch:  5 /  10  Train Loss :  0.3701860795696266\n",
            "Iter : 29000 at epoch:  5 /  10  Train Loss :  0.38628413977473974\n",
            "Iter : 30000 at epoch:  5 /  10  Train Loss :  0.4051538593545556\n",
            "Iter : 31000 at epoch:  5 /  10  Train Loss :  0.3772449596407823\n",
            "Iter : 32000 at epoch:  5 /  10  Train Loss :  0.4183576868898235\n",
            "Iter : 33000 at epoch:  5 /  10  Train Loss :  0.37168899502558633\n",
            "Iter : 34000 at epoch:  5 /  10  Train Loss :  0.40008334975410254\n",
            "Iter : 35000 at epoch:  5 /  10  Train Loss :  0.35959971279557795\n",
            "Iter : 36000 at epoch:  5 /  10  Train Loss :  0.385462193813175\n",
            "Iter : 37000 at epoch:  5 /  10  Train Loss :  0.3694030904634856\n",
            "Iter : 38000 at epoch:  5 /  10  Train Loss :  0.3733753011985682\n",
            "Iter : 39000 at epoch:  5 /  10  Train Loss :  0.3748963814415038\n",
            "Iter : 40000 at epoch:  5 /  10  Train Loss :  0.3961954227620736\n",
            "Iter : 41000 at epoch:  5 /  10  Train Loss :  0.38023062208853664\n",
            "Iter : 42000 at epoch:  5 /  10  Train Loss :  0.3652729664896615\n",
            "Iter : 43000 at epoch:  5 /  10  Train Loss :  0.39043280235817657\n",
            "Iter : 44000 at epoch:  5 /  10  Train Loss :  0.3726995304855518\n",
            "Iter : 45000 at epoch:  5 /  10  Train Loss :  0.38507787842210384\n",
            "Iter : 46000 at epoch:  5 /  10  Train Loss :  0.36410849284846336\n",
            "Iter : 47000 at epoch:  5 /  10  Train Loss :  0.4053100366103463\n",
            "Iter : 48000 at epoch:  5 /  10  Train Loss :  0.4070881344792433\n",
            "Iter : 49000 at epoch:  5 /  10  Train Loss :  0.3936001873956993\n",
            "Iter : 50000 at epoch:  5 /  10  Train Loss :  0.40750405606301504\n",
            "Iter : 51000 at epoch:  5 /  10  Train Loss :  0.3875146889742464\n",
            "Iter : 52000 at epoch:  5 /  10  Train Loss :  0.3851010947818868\n",
            "Iter : 53000 at epoch:  5 /  10  Train Loss :  0.3860695553291589\n",
            "Iter : 54000 at epoch:  5 /  10  Train Loss :  0.3801867518774234\n",
            "Iter : 55000 at epoch:  5 /  10  Train Loss :  0.38519009460462256\n",
            "Iter : 56000 at epoch:  5 /  10  Train Loss :  0.3762095842640847\n",
            "Iter : 57000 at epoch:  5 /  10  Train Loss :  0.40833593673631546\n",
            "Iter : 58000 at epoch:  5 /  10  Train Loss :  0.3910853466023691\n",
            "Iter : 59000 at epoch:  5 /  10  Train Loss :  0.3881729512498714\n",
            "Iter : 60000 at epoch:  5 /  10  Train Loss :  0.3779998524454422\n",
            "Iter : 61000 at epoch:  5 /  10  Train Loss :  0.39945293493475764\n",
            "Iter : 62000 at epoch:  5 /  10  Train Loss :  0.3692595361517742\n",
            "Iter : 63000 at epoch:  5 /  10  Train Loss :  0.3644242078163661\n",
            "Iter : 64000 at epoch:  5 /  10  Train Loss :  0.3863744278727099\n",
            "Iter : 65000 at epoch:  5 /  10  Train Loss :  0.40354895435925575\n",
            "Iter : 66000 at epoch:  5 /  10  Train Loss :  0.383636346695479\n",
            "Iter : 67000 at epoch:  5 /  10  Train Loss :  0.394872286377009\n",
            "Iter : 68000 at epoch:  5 /  10  Train Loss :  0.3623227338925935\n",
            "Iter : 69000 at epoch:  5 /  10  Train Loss :  0.3818231925377622\n",
            "Iter : 70000 at epoch:  5 /  10  Train Loss :  0.38864017963269726\n",
            "Iter : 71000 at epoch:  5 /  10  Train Loss :  0.3842972098514438\n",
            "Iter : 72000 at epoch:  5 /  10  Train Loss :  0.39087671038927513\n",
            "Iter : 73000 at epoch:  5 /  10  Train Loss :  0.37273846854362636\n",
            "Iter : 74000 at epoch:  5 /  10  Train Loss :  0.3862214295347221\n",
            "Iter:  75000 at epoch:  5 / 10  train loss : 0.382783676401712 Train acc:  82.9084451460142  val loss:  tensor(0.3809)  val acc:  83.15091559671647\n",
            "Iter : 76000 at epoch:  5 /  10  Train Loss :  0.36575079287961126\n",
            "Iter : 1000 at epoch:  6 /  10  Train Loss :  0.36347516243951394\n",
            "Iter : 2000 at epoch:  6 /  10  Train Loss :  0.42187055813334884\n",
            "Iter : 3000 at epoch:  6 /  10  Train Loss :  0.38029338664188983\n",
            "Iter : 4000 at epoch:  6 /  10  Train Loss :  0.37376097281090914\n",
            "Iter : 5000 at epoch:  6 /  10  Train Loss :  0.4000517590078525\n",
            "Iter : 6000 at epoch:  6 /  10  Train Loss :  0.3640890166214667\n",
            "Iter : 7000 at epoch:  6 /  10  Train Loss :  0.381435797277838\n",
            "Iter : 8000 at epoch:  6 /  10  Train Loss :  0.4178418381283991\n",
            "Iter : 9000 at epoch:  6 /  10  Train Loss :  0.3889985031699762\n",
            "Iter : 10000 at epoch:  6 /  10  Train Loss :  0.37836353916674853\n",
            "Iter : 11000 at epoch:  6 /  10  Train Loss :  0.3632293782052584\n",
            "Iter : 12000 at epoch:  6 /  10  Train Loss :  0.3771035522953607\n",
            "Iter : 13000 at epoch:  6 /  10  Train Loss :  0.40040966162132097\n",
            "Iter : 14000 at epoch:  6 /  10  Train Loss :  0.37645759191038086\n",
            "Iter : 15000 at epoch:  6 /  10  Train Loss :  0.3864998457287438\n",
            "Iter : 16000 at epoch:  6 /  10  Train Loss :  0.41282833377225325\n",
            "Iter : 17000 at epoch:  6 /  10  Train Loss :  0.4060938649829477\n",
            "Iter : 18000 at epoch:  6 /  10  Train Loss :  0.39248074373370034\n",
            "Iter : 19000 at epoch:  6 /  10  Train Loss :  0.38159765366232024\n",
            "Iter : 20000 at epoch:  6 /  10  Train Loss :  0.41218082021642477\n",
            "Iter : 21000 at epoch:  6 /  10  Train Loss :  0.3443830056572333\n",
            "Iter : 22000 at epoch:  6 /  10  Train Loss :  0.38634950353950265\n",
            "Iter : 23000 at epoch:  6 /  10  Train Loss :  0.39380380903091283\n",
            "Iter : 24000 at epoch:  6 /  10  Train Loss :  0.363918277584482\n",
            "Iter : 25000 at epoch:  6 /  10  Train Loss :  0.37600282662408424\n",
            "Iter : 26000 at epoch:  6 /  10  Train Loss :  0.364947443428915\n",
            "Iter : 27000 at epoch:  6 /  10  Train Loss :  0.382599121669773\n",
            "Iter : 28000 at epoch:  6 /  10  Train Loss :  0.41649569998960945\n",
            "Iter : 29000 at epoch:  6 /  10  Train Loss :  0.37429260854655877\n",
            "Iter : 30000 at epoch:  6 /  10  Train Loss :  0.38172885167924686\n",
            "Iter : 31000 at epoch:  6 /  10  Train Loss :  0.3788119102404453\n",
            "Iter : 32000 at epoch:  6 /  10  Train Loss :  0.3388244040007703\n",
            "Iter : 33000 at epoch:  6 /  10  Train Loss :  0.3689275310258381\n",
            "Iter : 34000 at epoch:  6 /  10  Train Loss :  0.3591365739037283\n",
            "Iter : 35000 at epoch:  6 /  10  Train Loss :  0.3816296790251508\n",
            "Iter : 36000 at epoch:  6 /  10  Train Loss :  0.3802238418213092\n",
            "Iter : 37000 at epoch:  6 /  10  Train Loss :  0.38699491783976553\n",
            "Iter : 38000 at epoch:  6 /  10  Train Loss :  0.4051717762858607\n",
            "Iter : 39000 at epoch:  6 /  10  Train Loss :  0.39469017840689047\n",
            "Iter : 40000 at epoch:  6 /  10  Train Loss :  0.3523312681131065\n",
            "Iter : 41000 at epoch:  6 /  10  Train Loss :  0.36609195844875647\n",
            "Iter : 42000 at epoch:  6 /  10  Train Loss :  0.432479058300145\n",
            "Iter : 43000 at epoch:  6 /  10  Train Loss :  0.39793511587707325\n",
            "Iter : 44000 at epoch:  6 /  10  Train Loss :  0.34228219063859433\n",
            "Iter : 45000 at epoch:  6 /  10  Train Loss :  0.36720527889113874\n",
            "Iter : 46000 at epoch:  6 /  10  Train Loss :  0.37446167729515584\n",
            "Iter : 47000 at epoch:  6 /  10  Train Loss :  0.38663590158987793\n",
            "Iter : 48000 at epoch:  6 /  10  Train Loss :  0.3584474087522831\n",
            "Iter : 49000 at epoch:  6 /  10  Train Loss :  0.38232146500423553\n",
            "Iter : 50000 at epoch:  6 /  10  Train Loss :  0.35518217272590846\n",
            "Iter : 51000 at epoch:  6 /  10  Train Loss :  0.40124727217666806\n",
            "Iter : 52000 at epoch:  6 /  10  Train Loss :  0.38560633902437985\n",
            "Iter : 53000 at epoch:  6 /  10  Train Loss :  0.3591148910545744\n",
            "Iter : 54000 at epoch:  6 /  10  Train Loss :  0.36853352707019077\n",
            "Iter : 55000 at epoch:  6 /  10  Train Loss :  0.41070746469171715\n",
            "Iter : 56000 at epoch:  6 /  10  Train Loss :  0.36400586713012306\n",
            "Iter : 57000 at epoch:  6 /  10  Train Loss :  0.38851092831441203\n",
            "Iter : 58000 at epoch:  6 /  10  Train Loss :  0.3605422134124674\n",
            "Iter : 59000 at epoch:  6 /  10  Train Loss :  0.367005536633078\n",
            "Iter : 60000 at epoch:  6 /  10  Train Loss :  0.4042952201124281\n",
            "Iter : 61000 at epoch:  6 /  10  Train Loss :  0.4000752978096716\n",
            "Iter : 62000 at epoch:  6 /  10  Train Loss :  0.3918324774377979\n",
            "Iter : 63000 at epoch:  6 /  10  Train Loss :  0.4016101265826728\n",
            "Iter : 64000 at epoch:  6 /  10  Train Loss :  0.3831881857011467\n",
            "Iter : 65000 at epoch:  6 /  10  Train Loss :  0.3588277736585587\n",
            "Iter : 66000 at epoch:  6 /  10  Train Loss :  0.3875747855287045\n",
            "Iter : 67000 at epoch:  6 /  10  Train Loss :  0.3311852250224911\n",
            "Iter : 68000 at epoch:  6 /  10  Train Loss :  0.4103627866390161\n",
            "Iter : 69000 at epoch:  6 /  10  Train Loss :  0.4109048942378722\n",
            "Iter : 70000 at epoch:  6 /  10  Train Loss :  0.40238102500606326\n",
            "Iter : 71000 at epoch:  6 /  10  Train Loss :  0.37983685154328123\n",
            "Iter : 72000 at epoch:  6 /  10  Train Loss :  0.37633399962168185\n",
            "Iter : 73000 at epoch:  6 /  10  Train Loss :  0.37591593082388863\n",
            "Iter : 74000 at epoch:  6 /  10  Train Loss :  0.3965807079183869\n",
            "Iter:  75000 at epoch:  6 / 10  train loss : 0.3993016054215841 Train acc:  82.98474085766904  val loss:  tensor(0.3780)  val acc:  83.54030730372554\n",
            "Iter : 76000 at epoch:  6 /  10  Train Loss :  0.3942557857572101\n",
            "Iter : 1000 at epoch:  7 /  10  Train Loss :  0.3784113421016373\n",
            "Iter : 2000 at epoch:  7 /  10  Train Loss :  0.40522505292668937\n",
            "Iter : 3000 at epoch:  7 /  10  Train Loss :  0.3904479508027434\n",
            "Iter : 4000 at epoch:  7 /  10  Train Loss :  0.37832336118631066\n",
            "Iter : 5000 at epoch:  7 /  10  Train Loss :  0.4217858852380887\n",
            "Iter : 6000 at epoch:  7 /  10  Train Loss :  0.4038411045121029\n",
            "Iter : 7000 at epoch:  7 /  10  Train Loss :  0.38938835060363636\n",
            "Iter : 8000 at epoch:  7 /  10  Train Loss :  0.38811160137644035\n",
            "Iter : 9000 at epoch:  7 /  10  Train Loss :  0.38129660981195046\n",
            "Iter : 10000 at epoch:  7 /  10  Train Loss :  0.3669251139909029\n",
            "Iter : 11000 at epoch:  7 /  10  Train Loss :  0.37352012237580495\n",
            "Iter : 12000 at epoch:  7 /  10  Train Loss :  0.3588671099152416\n",
            "Iter : 13000 at epoch:  7 /  10  Train Loss :  0.3960549251672346\n",
            "Iter : 14000 at epoch:  7 /  10  Train Loss :  0.3542415362105239\n",
            "Iter : 15000 at epoch:  7 /  10  Train Loss :  0.37422579037072135\n",
            "Iter : 16000 at epoch:  7 /  10  Train Loss :  0.3648080984163098\n",
            "Iter : 17000 at epoch:  7 /  10  Train Loss :  0.3454440842238255\n",
            "Iter : 18000 at epoch:  7 /  10  Train Loss :  0.38448072600504385\n",
            "Iter : 19000 at epoch:  7 /  10  Train Loss :  0.34916664826637134\n",
            "Iter : 20000 at epoch:  7 /  10  Train Loss :  0.38217974387202414\n",
            "Iter : 21000 at epoch:  7 /  10  Train Loss :  0.38963184216152874\n",
            "Iter : 22000 at epoch:  7 /  10  Train Loss :  0.3725472480668686\n",
            "Iter : 23000 at epoch:  7 /  10  Train Loss :  0.3897433165339753\n",
            "Iter : 24000 at epoch:  7 /  10  Train Loss :  0.37570779033424334\n",
            "Iter : 25000 at epoch:  7 /  10  Train Loss :  0.3303525106753223\n",
            "Iter : 26000 at epoch:  7 /  10  Train Loss :  0.4027401955560781\n",
            "Iter : 27000 at epoch:  7 /  10  Train Loss :  0.39343881388986485\n",
            "Iter : 28000 at epoch:  7 /  10  Train Loss :  0.343647244222695\n",
            "Iter : 29000 at epoch:  7 /  10  Train Loss :  0.40050834431126714\n",
            "Iter : 30000 at epoch:  7 /  10  Train Loss :  0.39324059019936247\n",
            "Iter : 31000 at epoch:  7 /  10  Train Loss :  0.35432224288303404\n",
            "Iter : 32000 at epoch:  7 /  10  Train Loss :  0.3782014592147898\n",
            "Iter : 33000 at epoch:  7 /  10  Train Loss :  0.3785012106318027\n",
            "Iter : 34000 at epoch:  7 /  10  Train Loss :  0.40086824155552314\n",
            "Iter : 35000 at epoch:  7 /  10  Train Loss :  0.4137256411248818\n",
            "Iter : 36000 at epoch:  7 /  10  Train Loss :  0.37451759265945295\n",
            "Iter : 37000 at epoch:  7 /  10  Train Loss :  0.3836254421295598\n",
            "Iter : 38000 at epoch:  7 /  10  Train Loss :  0.40526029326859864\n",
            "Iter : 39000 at epoch:  7 /  10  Train Loss :  0.3543123322718311\n",
            "Iter : 40000 at epoch:  7 /  10  Train Loss :  0.3873966193723027\n",
            "Iter : 41000 at epoch:  7 /  10  Train Loss :  0.3479695812715217\n",
            "Iter : 42000 at epoch:  7 /  10  Train Loss :  0.3892576818498783\n",
            "Iter : 43000 at epoch:  7 /  10  Train Loss :  0.3664885912332684\n",
            "Iter : 44000 at epoch:  7 /  10  Train Loss :  0.37419007778773083\n",
            "Iter : 45000 at epoch:  7 /  10  Train Loss :  0.38516275517549364\n",
            "Iter : 46000 at epoch:  7 /  10  Train Loss :  0.39146931847720406\n",
            "Iter : 47000 at epoch:  7 /  10  Train Loss :  0.3820089957374148\n",
            "Iter : 48000 at epoch:  7 /  10  Train Loss :  0.41442731285630724\n",
            "Iter : 49000 at epoch:  7 /  10  Train Loss :  0.36671225539618174\n",
            "Iter : 50000 at epoch:  7 /  10  Train Loss :  0.3632245735025499\n",
            "Iter : 51000 at epoch:  7 /  10  Train Loss :  0.36477723877714013\n",
            "Iter : 52000 at epoch:  7 /  10  Train Loss :  0.39426020224159586\n",
            "Iter : 53000 at epoch:  7 /  10  Train Loss :  0.4053761106920429\n",
            "Iter : 54000 at epoch:  7 /  10  Train Loss :  0.3858961248397827\n",
            "Iter : 55000 at epoch:  7 /  10  Train Loss :  0.40305880090384744\n",
            "Iter : 56000 at epoch:  7 /  10  Train Loss :  0.3930314260357991\n",
            "Iter : 57000 at epoch:  7 /  10  Train Loss :  0.35917819347558544\n",
            "Iter : 58000 at epoch:  7 /  10  Train Loss :  0.35854075613059105\n",
            "Iter : 59000 at epoch:  7 /  10  Train Loss :  0.3920245223683305\n",
            "Iter : 60000 at epoch:  7 /  10  Train Loss :  0.3622214022716507\n",
            "Iter : 61000 at epoch:  7 /  10  Train Loss :  0.388049410183914\n",
            "Iter : 62000 at epoch:  7 /  10  Train Loss :  0.3823715542072896\n",
            "Iter : 63000 at epoch:  7 /  10  Train Loss :  0.3751689763283357\n",
            "Iter : 64000 at epoch:  7 /  10  Train Loss :  0.3687503494210541\n",
            "Iter : 65000 at epoch:  7 /  10  Train Loss :  0.40549528517550787\n",
            "Iter : 66000 at epoch:  7 /  10  Train Loss :  0.39275156092690305\n",
            "Iter : 67000 at epoch:  7 /  10  Train Loss :  0.40242331329383885\n",
            "Iter : 68000 at epoch:  7 /  10  Train Loss :  0.4017878007129766\n",
            "Iter : 69000 at epoch:  7 /  10  Train Loss :  0.3536875270574819\n",
            "Iter : 70000 at epoch:  7 /  10  Train Loss :  0.3497285548807122\n",
            "Iter : 71000 at epoch:  7 /  10  Train Loss :  0.363397911433829\n",
            "Iter : 72000 at epoch:  7 /  10  Train Loss :  0.36734237532061526\n",
            "Iter : 73000 at epoch:  7 /  10  Train Loss :  0.3673692856251728\n",
            "Iter : 74000 at epoch:  7 /  10  Train Loss :  0.3847705216805916\n",
            "Iter:  75000 at epoch:  7 / 10  train loss : 0.3542235183538869 Train acc:  83.22415153906867  val loss:  tensor(0.3748)  val acc:  83.54030730372554\n",
            "Iter : 76000 at epoch:  7 /  10  Train Loss :  0.4077918615620583\n",
            "Iter : 1000 at epoch:  8 /  10  Train Loss :  0.3505873045385815\n",
            "Iter : 2000 at epoch:  8 /  10  Train Loss :  0.3619919806814287\n",
            "Iter : 3000 at epoch:  8 /  10  Train Loss :  0.3684438405861147\n",
            "Iter : 4000 at epoch:  8 /  10  Train Loss :  0.38602560328133406\n",
            "Iter : 5000 at epoch:  8 /  10  Train Loss :  0.3636391330033075\n",
            "Iter : 6000 at epoch:  8 /  10  Train Loss :  0.34878194793337025\n",
            "Iter : 7000 at epoch:  8 /  10  Train Loss :  0.384487925746711\n",
            "Iter : 8000 at epoch:  8 /  10  Train Loss :  0.3831516187125817\n",
            "Iter : 9000 at epoch:  8 /  10  Train Loss :  0.37824048403138294\n",
            "Iter : 10000 at epoch:  8 /  10  Train Loss :  0.3588870403212495\n",
            "Iter : 11000 at epoch:  8 /  10  Train Loss :  0.3899173741682898\n",
            "Iter : 12000 at epoch:  8 /  10  Train Loss :  0.35348463513096795\n",
            "Iter : 13000 at epoch:  8 /  10  Train Loss :  0.37291564378701153\n",
            "Iter : 14000 at epoch:  8 /  10  Train Loss :  0.39624579905229623\n",
            "Iter : 15000 at epoch:  8 /  10  Train Loss :  0.37370501141389834\n",
            "Iter : 16000 at epoch:  8 /  10  Train Loss :  0.39453680352517406\n",
            "Iter : 17000 at epoch:  8 /  10  Train Loss :  0.40141998809506185\n",
            "Iter : 18000 at epoch:  8 /  10  Train Loss :  0.3578385333635379\n",
            "Iter : 19000 at epoch:  8 /  10  Train Loss :  0.4036851380313747\n",
            "Iter : 20000 at epoch:  8 /  10  Train Loss :  0.39033728403924034\n",
            "Iter : 21000 at epoch:  8 /  10  Train Loss :  0.39584658929752187\n",
            "Iter : 22000 at epoch:  8 /  10  Train Loss :  0.3798134493846446\n",
            "Iter : 23000 at epoch:  8 /  10  Train Loss :  0.41474766879552044\n",
            "Iter : 24000 at epoch:  8 /  10  Train Loss :  0.36727998015563934\n",
            "Iter : 25000 at epoch:  8 /  10  Train Loss :  0.36972264815773814\n",
            "Iter : 26000 at epoch:  8 /  10  Train Loss :  0.37548589099897073\n",
            "Iter : 27000 at epoch:  8 /  10  Train Loss :  0.38937536815577184\n",
            "Iter : 28000 at epoch:  8 /  10  Train Loss :  0.39020376481930724\n",
            "Iter : 29000 at epoch:  8 /  10  Train Loss :  0.3893588829699438\n",
            "Iter : 30000 at epoch:  8 /  10  Train Loss :  0.3641053266862873\n",
            "Iter : 31000 at epoch:  8 /  10  Train Loss :  0.36994270787225103\n",
            "Iter : 32000 at epoch:  8 /  10  Train Loss :  0.4242479980539065\n",
            "Iter : 33000 at epoch:  8 /  10  Train Loss :  0.3631389387876261\n",
            "Iter : 34000 at epoch:  8 /  10  Train Loss :  0.4009048890257254\n",
            "Iter : 35000 at epoch:  8 /  10  Train Loss :  0.3843618207303807\n",
            "Iter : 36000 at epoch:  8 /  10  Train Loss :  0.37333789354050534\n",
            "Iter : 37000 at epoch:  8 /  10  Train Loss :  0.35037059874483384\n",
            "Iter : 38000 at epoch:  8 /  10  Train Loss :  0.32749278271524235\n",
            "Iter : 39000 at epoch:  8 /  10  Train Loss :  0.3720273448675871\n",
            "Iter : 40000 at epoch:  8 /  10  Train Loss :  0.35654625096404924\n",
            "Iter : 41000 at epoch:  8 /  10  Train Loss :  0.386531317031011\n",
            "Iter : 42000 at epoch:  8 /  10  Train Loss :  0.3861095523366239\n",
            "Iter : 43000 at epoch:  8 /  10  Train Loss :  0.35752774082496763\n",
            "Iter : 44000 at epoch:  8 /  10  Train Loss :  0.3578498065334279\n",
            "Iter : 45000 at epoch:  8 /  10  Train Loss :  0.36034796459041535\n",
            "Iter : 46000 at epoch:  8 /  10  Train Loss :  0.395360649527749\n",
            "Iter : 47000 at epoch:  8 /  10  Train Loss :  0.3640639450035524\n",
            "Iter : 48000 at epoch:  8 /  10  Train Loss :  0.37614540579495953\n",
            "Iter : 49000 at epoch:  8 /  10  Train Loss :  0.4026340011244174\n",
            "Iter : 50000 at epoch:  8 /  10  Train Loss :  0.3893682778046932\n",
            "Iter : 51000 at epoch:  8 /  10  Train Loss :  0.37172166141611523\n",
            "Iter : 52000 at epoch:  8 /  10  Train Loss :  0.385126152421115\n",
            "Iter : 53000 at epoch:  8 /  10  Train Loss :  0.3630980307261925\n",
            "Iter : 54000 at epoch:  8 /  10  Train Loss :  0.3666511607049033\n",
            "Iter : 55000 at epoch:  8 /  10  Train Loss :  0.37761895910510795\n",
            "Iter : 56000 at epoch:  8 /  10  Train Loss :  0.3856763095601928\n",
            "Iter : 57000 at epoch:  8 /  10  Train Loss :  0.3402949653619435\n",
            "Iter : 58000 at epoch:  8 /  10  Train Loss :  0.3577377770887688\n",
            "Iter : 59000 at epoch:  8 /  10  Train Loss :  0.4194255312518217\n",
            "Iter : 60000 at epoch:  8 /  10  Train Loss :  0.3506674658493139\n",
            "Iter : 61000 at epoch:  8 /  10  Train Loss :  0.38401805142173545\n",
            "Iter : 62000 at epoch:  8 /  10  Train Loss :  0.3696277579043526\n",
            "Iter : 63000 at epoch:  8 /  10  Train Loss :  0.4069142849689815\n",
            "Iter : 64000 at epoch:  8 /  10  Train Loss :  0.388912062388612\n",
            "Iter : 65000 at epoch:  8 /  10  Train Loss :  0.39079046232951803\n",
            "Iter : 66000 at epoch:  8 /  10  Train Loss :  0.36538218113570475\n",
            "Iter : 67000 at epoch:  8 /  10  Train Loss :  0.38423690206068567\n",
            "Iter : 68000 at epoch:  8 /  10  Train Loss :  0.3557863271338865\n",
            "Iter : 69000 at epoch:  8 /  10  Train Loss :  0.3792743453253061\n",
            "Iter : 70000 at epoch:  8 /  10  Train Loss :  0.39172262899647464\n",
            "Iter : 71000 at epoch:  8 /  10  Train Loss :  0.3811862299297936\n",
            "Iter : 72000 at epoch:  8 /  10  Train Loss :  0.40393588252202606\n",
            "Iter : 73000 at epoch:  8 /  10  Train Loss :  0.3878120209344197\n",
            "Iter : 74000 at epoch:  8 /  10  Train Loss :  0.38363382838806137\n",
            "Iter:  75000 at epoch:  8 / 10  train loss : 0.3676324529810809 Train acc:  83.39252828203104  val loss:  tensor(0.3731)  val acc:  83.5613555041044\n",
            "Iter : 76000 at epoch:  8 /  10  Train Loss :  0.36505916388868354\n",
            "Iter : 1000 at epoch:  9 /  10  Train Loss :  0.4133461732126307\n",
            "Iter : 2000 at epoch:  9 /  10  Train Loss :  0.36524293628963644\n",
            "Iter : 3000 at epoch:  9 /  10  Train Loss :  0.3798695766734891\n",
            "Iter : 4000 at epoch:  9 /  10  Train Loss :  0.4015033788974397\n",
            "Iter : 5000 at epoch:  9 /  10  Train Loss :  0.3613347000400536\n",
            "Iter : 6000 at epoch:  9 /  10  Train Loss :  0.3921099364128895\n",
            "Iter : 7000 at epoch:  9 /  10  Train Loss :  0.3624313359896187\n",
            "Iter : 8000 at epoch:  9 /  10  Train Loss :  0.37124435292673297\n",
            "Iter : 9000 at epoch:  9 /  10  Train Loss :  0.3880653818782885\n",
            "Iter : 10000 at epoch:  9 /  10  Train Loss :  0.3565896716278512\n",
            "Iter : 11000 at epoch:  9 /  10  Train Loss :  0.3512693806765601\n",
            "Iter : 12000 at epoch:  9 /  10  Train Loss :  0.39281776851508765\n",
            "Iter : 13000 at epoch:  9 /  10  Train Loss :  0.37314219589345154\n",
            "Iter : 14000 at epoch:  9 /  10  Train Loss :  0.381985696019372\n",
            "Iter : 15000 at epoch:  9 /  10  Train Loss :  0.35073852840252223\n",
            "Iter : 16000 at epoch:  9 /  10  Train Loss :  0.3661401125269476\n",
            "Iter : 17000 at epoch:  9 /  10  Train Loss :  0.37716360116237774\n",
            "Iter : 18000 at epoch:  9 /  10  Train Loss :  0.3453959845448844\n",
            "Iter : 19000 at epoch:  9 /  10  Train Loss :  0.3800359760294668\n",
            "Iter : 20000 at epoch:  9 /  10  Train Loss :  0.3701071233248804\n",
            "Iter : 21000 at epoch:  9 /  10  Train Loss :  0.39596517927502284\n",
            "Iter : 22000 at epoch:  9 /  10  Train Loss :  0.36958624932123346\n",
            "Iter : 23000 at epoch:  9 /  10  Train Loss :  0.40029216090356934\n",
            "Iter : 24000 at epoch:  9 /  10  Train Loss :  0.37691452003084125\n",
            "Iter : 25000 at epoch:  9 /  10  Train Loss :  0.36747182987933047\n",
            "Iter : 26000 at epoch:  9 /  10  Train Loss :  0.35793643434415573\n",
            "Iter : 27000 at epoch:  9 /  10  Train Loss :  0.38809020959911866\n",
            "Iter : 28000 at epoch:  9 /  10  Train Loss :  0.3930888942766469\n",
            "Iter : 29000 at epoch:  9 /  10  Train Loss :  0.3788758231017273\n",
            "Iter : 30000 at epoch:  9 /  10  Train Loss :  0.37419660114403813\n",
            "Iter : 31000 at epoch:  9 /  10  Train Loss :  0.36307711112149993\n",
            "Iter : 32000 at epoch:  9 /  10  Train Loss :  0.367544041703688\n",
            "Iter : 33000 at epoch:  9 /  10  Train Loss :  0.3532274724091403\n",
            "Iter : 34000 at epoch:  9 /  10  Train Loss :  0.38733844908466564\n",
            "Iter : 35000 at epoch:  9 /  10  Train Loss :  0.3906223244785797\n",
            "Iter : 36000 at epoch:  9 /  10  Train Loss :  0.3988827044789214\n",
            "Iter : 37000 at epoch:  9 /  10  Train Loss :  0.3739696336768102\n",
            "Iter : 38000 at epoch:  9 /  10  Train Loss :  0.39416958622983655\n",
            "Iter : 39000 at epoch:  9 /  10  Train Loss :  0.37732112881192004\n",
            "Iter : 40000 at epoch:  9 /  10  Train Loss :  0.38429467171616855\n",
            "Iter : 41000 at epoch:  9 /  10  Train Loss :  0.40242295649694276\n",
            "Iter : 42000 at epoch:  9 /  10  Train Loss :  0.3931284419503063\n",
            "Iter : 43000 at epoch:  9 /  10  Train Loss :  0.3780766438655555\n",
            "Iter : 44000 at epoch:  9 /  10  Train Loss :  0.3767404971790966\n",
            "Iter : 45000 at epoch:  9 /  10  Train Loss :  0.3529433516827412\n",
            "Iter : 46000 at epoch:  9 /  10  Train Loss :  0.3789021200921852\n",
            "Iter : 47000 at epoch:  9 /  10  Train Loss :  0.3877749533688184\n",
            "Iter : 48000 at epoch:  9 /  10  Train Loss :  0.3680249721428845\n",
            "Iter : 49000 at epoch:  9 /  10  Train Loss :  0.3739414375247434\n",
            "Iter : 50000 at epoch:  9 /  10  Train Loss :  0.34247662143432533\n",
            "Iter : 51000 at epoch:  9 /  10  Train Loss :  0.37281109334365464\n",
            "Iter : 52000 at epoch:  9 /  10  Train Loss :  0.37151569938473405\n",
            "Iter : 53000 at epoch:  9 /  10  Train Loss :  0.3624930653760675\n",
            "Iter : 54000 at epoch:  9 /  10  Train Loss :  0.3861877130556386\n",
            "Iter : 55000 at epoch:  9 /  10  Train Loss :  0.3527423630990088\n",
            "Iter : 56000 at epoch:  9 /  10  Train Loss :  0.3433147975658067\n",
            "Iter : 57000 at epoch:  9 /  10  Train Loss :  0.4053675844557583\n",
            "Iter : 58000 at epoch:  9 /  10  Train Loss :  0.34917586867697537\n",
            "Iter : 59000 at epoch:  9 /  10  Train Loss :  0.36149528103950435\n",
            "Iter : 60000 at epoch:  9 /  10  Train Loss :  0.3814604806522839\n",
            "Iter : 61000 at epoch:  9 /  10  Train Loss :  0.38200021041883153\n",
            "Iter : 62000 at epoch:  9 /  10  Train Loss :  0.34887971993908284\n",
            "Iter : 63000 at epoch:  9 /  10  Train Loss :  0.37029062711959704\n",
            "Iter : 64000 at epoch:  9 /  10  Train Loss :  0.3447276856768876\n",
            "Iter : 65000 at epoch:  9 /  10  Train Loss :  0.37266017618891784\n",
            "Iter : 66000 at epoch:  9 /  10  Train Loss :  0.3765600596824661\n",
            "Iter : 67000 at epoch:  9 /  10  Train Loss :  0.3857615056850482\n",
            "Iter : 68000 at epoch:  9 /  10  Train Loss :  0.3910661903496366\n",
            "Iter : 69000 at epoch:  9 /  10  Train Loss :  0.3684541573000606\n",
            "Iter : 70000 at epoch:  9 /  10  Train Loss :  0.36795893630967474\n",
            "Iter : 71000 at epoch:  9 /  10  Train Loss :  0.3398421899531968\n",
            "Iter : 72000 at epoch:  9 /  10  Train Loss :  0.3745654039895162\n",
            "Iter : 73000 at epoch:  9 /  10  Train Loss :  0.41119069163966926\n",
            "Iter : 74000 at epoch:  9 /  10  Train Loss :  0.366975058701355\n",
            "Iter:  75000 at epoch:  9 / 10  train loss : 0.41130608089081944 Train acc:  83.51617995264404  val loss:  tensor(0.3701)  val acc:  83.6560724058093\n",
            "Iter : 76000 at epoch:  9 /  10  Train Loss :  0.37466446834709494\n",
            "Iter : 1000 at epoch:  10 /  10  Train Loss :  0.3967069275528193\n",
            "Iter : 2000 at epoch:  10 /  10  Train Loss :  0.3661280599858146\n",
            "Iter : 3000 at epoch:  10 /  10  Train Loss :  0.3805742461963091\n",
            "Iter : 4000 at epoch:  10 /  10  Train Loss :  0.35239698796882296\n",
            "Iter : 5000 at epoch:  10 /  10  Train Loss :  0.37965963704907335\n",
            "Iter : 6000 at epoch:  10 /  10  Train Loss :  0.3795779865698423\n",
            "Iter : 7000 at epoch:  10 /  10  Train Loss :  0.3743962059181649\n",
            "Iter : 8000 at epoch:  10 /  10  Train Loss :  0.34184809418744405\n",
            "Iter : 9000 at epoch:  10 /  10  Train Loss :  0.39059980095899666\n",
            "Iter : 10000 at epoch:  10 /  10  Train Loss :  0.37377754487958736\n",
            "Iter : 11000 at epoch:  10 /  10  Train Loss :  0.37841133871348576\n",
            "Iter : 12000 at epoch:  10 /  10  Train Loss :  0.38394500949000937\n",
            "Iter : 13000 at epoch:  10 /  10  Train Loss :  0.3650581005886197\n",
            "Iter : 14000 at epoch:  10 /  10  Train Loss :  0.3497606595492689\n",
            "Iter : 15000 at epoch:  10 /  10  Train Loss :  0.3901740322476253\n",
            "Iter : 16000 at epoch:  10 /  10  Train Loss :  0.38339586919476276\n",
            "Iter : 17000 at epoch:  10 /  10  Train Loss :  0.3627708160886541\n",
            "Iter : 18000 at epoch:  10 /  10  Train Loss :  0.3620202434505336\n",
            "Iter : 19000 at epoch:  10 /  10  Train Loss :  0.3697866405067034\n",
            "Iter : 20000 at epoch:  10 /  10  Train Loss :  0.38389608899713495\n",
            "Iter : 21000 at epoch:  10 /  10  Train Loss :  0.391175905717304\n",
            "Iter : 22000 at epoch:  10 /  10  Train Loss :  0.3456319675967097\n",
            "Iter : 23000 at epoch:  10 /  10  Train Loss :  0.3840685830563307\n",
            "Iter : 24000 at epoch:  10 /  10  Train Loss :  0.36091619430645366\n",
            "Iter : 25000 at epoch:  10 /  10  Train Loss :  0.3300796744469553\n",
            "Iter : 26000 at epoch:  10 /  10  Train Loss :  0.39398072468000467\n",
            "Iter : 27000 at epoch:  10 /  10  Train Loss :  0.35661076924414375\n",
            "Iter : 28000 at epoch:  10 /  10  Train Loss :  0.36996931010764095\n",
            "Iter : 29000 at epoch:  10 /  10  Train Loss :  0.37197748646955003\n",
            "Iter : 30000 at epoch:  10 /  10  Train Loss :  0.3760872960006818\n",
            "Iter : 31000 at epoch:  10 /  10  Train Loss :  0.3972437872446608\n",
            "Iter : 32000 at epoch:  10 /  10  Train Loss :  0.37513370942766777\n",
            "Iter : 33000 at epoch:  10 /  10  Train Loss :  0.3763874978229869\n",
            "Iter : 34000 at epoch:  10 /  10  Train Loss :  0.3933342851817142\n",
            "Iter : 35000 at epoch:  10 /  10  Train Loss :  0.37861713031702676\n",
            "Iter : 36000 at epoch:  10 /  10  Train Loss :  0.3996333892182447\n",
            "Iter : 37000 at epoch:  10 /  10  Train Loss :  0.37412121480563654\n",
            "Iter : 38000 at epoch:  10 /  10  Train Loss :  0.376286629864946\n",
            "Iter : 39000 at epoch:  10 /  10  Train Loss :  0.374983571480494\n",
            "Iter : 40000 at epoch:  10 /  10  Train Loss :  0.3639499817220494\n",
            "Iter : 41000 at epoch:  10 /  10  Train Loss :  0.3839544025957584\n",
            "Iter : 42000 at epoch:  10 /  10  Train Loss :  0.39045023835706527\n",
            "Iter : 43000 at epoch:  10 /  10  Train Loss :  0.35852240624744447\n",
            "Iter : 44000 at epoch:  10 /  10  Train Loss :  0.37780177882872523\n",
            "Iter : 45000 at epoch:  10 /  10  Train Loss :  0.3859989712014794\n",
            "Iter : 46000 at epoch:  10 /  10  Train Loss :  0.37579145544744097\n",
            "Iter : 47000 at epoch:  10 /  10  Train Loss :  0.35640272724605165\n",
            "Iter : 48000 at epoch:  10 /  10  Train Loss :  0.38179071588814256\n",
            "Iter : 49000 at epoch:  10 /  10  Train Loss :  0.38824280902789904\n",
            "Iter : 50000 at epoch:  10 /  10  Train Loss :  0.3953471128931269\n",
            "Iter : 51000 at epoch:  10 /  10  Train Loss :  0.37685918509354815\n",
            "Iter : 52000 at epoch:  10 /  10  Train Loss :  0.4016095277381828\n",
            "Iter : 53000 at epoch:  10 /  10  Train Loss :  0.35429345069197005\n",
            "Iter : 54000 at epoch:  10 /  10  Train Loss :  0.35411751435906624\n",
            "Iter : 55000 at epoch:  10 /  10  Train Loss :  0.3998408416812308\n",
            "Iter : 56000 at epoch:  10 /  10  Train Loss :  0.39777278927061704\n",
            "Iter : 57000 at epoch:  10 /  10  Train Loss :  0.3852908297327813\n",
            "Iter : 58000 at epoch:  10 /  10  Train Loss :  0.37699733772780747\n",
            "Iter : 59000 at epoch:  10 /  10  Train Loss :  0.3715961495495867\n",
            "Iter : 60000 at epoch:  10 /  10  Train Loss :  0.31993621289159635\n",
            "Iter : 61000 at epoch:  10 /  10  Train Loss :  0.36967238620575515\n",
            "Iter : 62000 at epoch:  10 /  10  Train Loss :  0.3685000031860545\n",
            "Iter : 63000 at epoch:  10 /  10  Train Loss :  0.37243558098655194\n",
            "Iter : 64000 at epoch:  10 /  10  Train Loss :  0.36489401295245627\n",
            "Iter : 65000 at epoch:  10 /  10  Train Loss :  0.3665746267333161\n",
            "Iter : 66000 at epoch:  10 /  10  Train Loss :  0.3501081770712044\n",
            "Iter : 67000 at epoch:  10 /  10  Train Loss :  0.3765418771372642\n",
            "Iter : 68000 at epoch:  10 /  10  Train Loss :  0.372437325285282\n",
            "Iter : 69000 at epoch:  10 /  10  Train Loss :  0.34495182589115575\n",
            "Iter : 70000 at epoch:  10 /  10  Train Loss :  0.3406305266940035\n",
            "Iter : 71000 at epoch:  10 /  10  Train Loss :  0.37085312645696106\n",
            "Iter : 72000 at epoch:  10 /  10  Train Loss :  0.3712854005480185\n",
            "Iter : 73000 at epoch:  10 /  10  Train Loss :  0.33892139738285915\n",
            "Iter : 74000 at epoch:  10 /  10  Train Loss :  0.3779084480158053\n",
            "Iter:  75000 at epoch:  10 / 10  train loss : 0.3875933366811369 Train acc:  83.46750855038148  val loss:  tensor(0.3702)  val acc:  83.87707850978742\n",
            "Iter : 76000 at epoch:  10 /  10  Train Loss :  0.3756348864894826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLIIa0ZyY98Z",
        "colab_type": "text"
      },
      "source": [
        "# **Visualize Loss and Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nopt6q7FhxE0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "3520221b-8b90-477e-cef0-d4724743d020"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "total_step_list=np.arange(len(train_loss_list))\n",
        "plt.plot(total_step_list,train_loss_list)\n",
        "plt.plot(total_step_list,val_loss_list)\n",
        "plt.title(\"LOSS\",fontsize=30)\n",
        "plt.xlabel('step',fontsize=20)\n",
        "plt.ylabel('loss',fontsize=20)\n",
        "plt.legend(['train','val'],loc='upper right')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4a10112a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1TMGWlbZDZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "a944155f-01ec-4175-8148-9f5212869d5f"
      },
      "source": [
        "\n",
        "total_step_list=np.arange(len(train_acc_list))\n",
        "plt.plot(total_step_list,train_acc_list)\n",
        "plt.plot(total_step_list,val_acc_list)\n",
        "plt.title(\"ACCURACY\",fontsize=30)\n",
        "plt.xlabel('step',fontsize=20)\n",
        "plt.ylabel('accuracy',fontsize=20)\n",
        "plt.legend(['train','val'],loc='lower right')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4a100b1710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUQgvr1yZDxl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "77a9e054-96c4-4bd4-beca-c708fd8cc259"
      },
      "source": [
        "print(\"Test Accuracy : \",evaluate(X_test,y_test)[1],\"%\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy :  83.58413132694939 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}